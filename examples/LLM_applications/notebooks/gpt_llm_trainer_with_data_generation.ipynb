{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wM8MRkf8Dr94"
      },
      "source": [
        "## Describe your model -> fine-tuned LLaMA 2\n",
        "By Matt Shumer (https://twitter.com/mattshumer_)\n",
        "https://github.com/mshumer/gpt-llm-trainer\n",
        "\n",
        "The goal of this notebook is to experiment with a new way to make it very easy to build a task-specific model for your use-case.\n",
        "\n",
        "First, use the best GPU available (go to Runtime -> change runtime type)\n",
        "\n",
        "To create your model, just go to the first code cell, and describe the model you want to build in the prompt. Be descriptive and clear.\n",
        "\n",
        "Select a temperature (high=creative, low=precise), and the number of training examples to generate to train the model. From there, just run all the cells.\n",
        "\n",
        "You can change the model you want to fine-tune by changing `model_name` in the `Define Hyperparameters` cell."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Way3_PuPpIuE"
      },
      "source": [
        "# Data generation step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-3DvlIpVSl"
      },
      "source": [
        "Write your prompt here. Make it as descriptive as possible!\n",
        "\n",
        "Then, choose the temperature (between 0 and 1) to use when generating data. Lower values are great for precise tasks, like writing code, whereas larger values are better for creative tasks, like writing stories.\n",
        "\n",
        "Finally, choose how many examples you want to generate. The more you generate, a) the longer it takes and b) the more expensive data generation will be. But generally, more examples will lead to a higher-quality model. 100 is usually the minimum to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os,sys\n",
        "sys.path.insert(0,'../')\n",
        "sys.path.insert(0,'../../../libs')\n",
        "from utils import load_json\n",
        "import openai\n",
        "import random\n",
        "import tqdm\n",
        "import config\n",
        "\n",
        "openai_key = load_json('/home/chengyu.huang/project/Fund_projects/openai_key.json')['ChatGPT1']['API_KEY']\n",
        "openai.api_key = openai_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "R7WKZyxtpUPS"
      },
      "outputs": [],
      "source": [
        "### defined highlevel objective \n",
        "prompt = '''A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, \n",
        "step-by-step thought out response.'''\n",
        "temperature = .4\n",
        "number_of_examples = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snNou5PrIci"
      },
      "source": [
        "Run this to generate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Rdsd82ngpHCG"
      },
      "outputs": [],
      "source": [
        "def generate_example(prompt, prev_examples, temperature=.5):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 10:\n",
        "            prev_examples = random.sample(prev_examples, 10)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  ## here we use 3.5 for testing ,\"gpt-3.5-turbo\"\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1354,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:43<00:00,  1.04s/it]\n"
          ]
        }
      ],
      "source": [
        "# Generate examples\n",
        "prev_examples = []\n",
        "for i in tqdm.tqdm(range(number_of_examples)):\n",
        "    #print(f'Generating example {i}')\n",
        "    try:\n",
        "        example = generate_example(prompt, prev_examples, temperature)\n",
        "        prev_examples.append(example)\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC6iJzXjugJ-"
      },
      "source": [
        "We also need to generate a system message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "xMcfhW6Guh2E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The system message is: `Given a puzzle-like reasoning-heavy question in English, you will provide a well-reasoned, step-by-step thought out response.`. Feel free to re-run this cell if you want a better result.\n"
          ]
        }
      ],
      "source": [
        "def generate_system_message(prompt):\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  ## here we use 3.5 for testing ,\"gpt-3.5-turbo\"\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt.strip(),\n",
        "          }\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "system_message = generate_system_message(prompt)\n",
        "\n",
        "print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6BqZ-hjseBF"
      },
      "source": [
        "Now let's put our examples into a dataframe and turn them into a final pair of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "7CEdkYeRsdmB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt\n",
            "-----------\n",
            "What is the capital of France?\n",
            "-----------\n",
            "\n",
            "response\n",
            "-----------\n",
            "The capital of France is Paris. It is one of the most famous cities in the world and is known for its rich history, stunning architecture, and cultural attractions. Paris is home to iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. It is also a major center for art, fashion, and cuisine.\n",
            "There are 8 successfully-generated examples. Here are the first few:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>The capital of France is Paris. It is one of t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does gravity work?</td>\n",
              "      <td>Gravity is a fundamental force in nature that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the process of photosynthesis?</td>\n",
              "      <td>Photosynthesis is the process by which plants,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does the immune system work?</td>\n",
              "      <td>The immune system is a complex network of cell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the internet work?</td>\n",
              "      <td>The internet is a global network of interconne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   prompt  \\\n",
              "0          What is the capital of France?   \n",
              "1                  How does gravity work?   \n",
              "2  What is the process of photosynthesis?   \n",
              "3        How does the immune system work?   \n",
              "4             How does the internet work?   \n",
              "\n",
              "                                            response  \n",
              "0  The capital of France is Paris. It is one of t...  \n",
              "1  Gravity is a fundamental force in nature that ...  \n",
              "2  Photosynthesis is the process by which plants,...  \n",
              "3  The immune system is a complex network of cell...  \n",
              "4  The internet is a global network of interconne...  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "print(prev_examples[0])\n",
        "# Initialize lists to store prompts and responses\n",
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "# Parse out prompts and responses from examples\n",
        "for example in prev_examples:\n",
        "  try:\n",
        "    split_example = example.split('-----------')\n",
        "    prompts.append(split_example[1].strip())\n",
        "    responses.append(split_example[3].strip())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'prompt': prompts,\n",
        "    'response': responses\n",
        "})\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "print('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "## save training data \n",
        "df.to_csv(os.path.join(config.data_folder3,'llm_simple_train.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-8dt5qqtpgM"
      },
      "source": [
        "Split into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "GFPEn1omtrXM"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets, with 90% in the train set\n",
        "train_df = df.sample(frac=0.9, random_state=42)\n",
        "test_df = df.drop(train_df.index)\n",
        "\n",
        "# Save the dataframes to .jsonl files\n",
        "train_df.to_json(os.path.join(config.data_folder3,'train.jsonl'), orient='records', lines=True)\n",
        "test_df.to_json(os.path.join(config.data_folder3,'test.jsonl'), orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbrFgrhG_xYi"
      },
      "source": [
        "# Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "lPG7wEPetFx2"
      },
      "outputs": [],
      "source": [
        "#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moVo0led-6tu"
      },
      "source": [
        "# Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "bqfbhUZI-4c_"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/llama-2-7b-chat-hf\" # use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", \n",
        "                                            # though keep in mind you'll need to pass a Hugging Face key argument\n",
        "dataset_name = os.path.join(config.data_folder3,\"train.jsonl\")\n",
        "new_model = os.path.join(config.data_folder3,\"llama-2-7b-custom\")\n",
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "output_dir = os.path.join(config.data_folder3,\"results\") ## change to local path\n",
        "num_train_epochs = 3\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"constant\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 25\n",
        "logging_steps = 5\n",
        "max_seq_length = None\n",
        "packing = False\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F-J5p5KS_MZY"
      },
      "source": [
        "# Load Datasets and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_dataset = load_dataset('json', data_files=os.path.join(config.data_folder3,'train.jsonl'), split=\"train\")\n",
        "valid_dataset = load_dataset('json', data_files=os.path.join(config.data_folder3,'test.jsonl'), split=\"train\")\n",
        "\n",
        "# Preprocess datasets\n",
        "train_dataset_mapped = train_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n",
        "valid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] <<SYS>>\n",
            "Given a puzzle-like reasoning-heavy question in English, you will provide a well-reasoned, step-by-step thought out response.\n",
            "<</SYS>>\n",
            "\n",
            "How does gravity work? [/INST] Gravity is a fundamental force in nature that attracts objects with mass towards each other. According to Einstein's theory of general relativity, gravity is the result of the curvature of spacetime caused by the presence of mass and energy. This curvature creates a gravitational field, which causes objects to move towards each other. The strength of gravity depends on the mass of the objects and the distance between them. For example, the larger the mass of an object, the stronger its gravitational pull. Gravity is what keeps planets in orbit around the sun and holds us to the Earth's surface.\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset_mapped[0]['text'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set up config and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qf1qxbiF-x6p"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    cache_dir = config.cache_dir\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"all\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=5  # Evaluate every 20 steps\n",
        ")\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_mapped,\n",
        "    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(new_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] <<SYS>>\n",
            "Given a puzzle-like reasoning-heavy question in English, you will provide a well-reasoned, step-by-step thought out response.\n",
            "<</SYS>>\n",
            "\n",
            "Write a function that reverses a string. [/INST]  Great! Let's break down the problem of reversing a string into smaller, manageable steps:\n",
            "\n",
            "Step 1: Understand the problem\n",
            "\n",
            "The problem statement asks us to write a function that reverses a given string. In other words, we need to take a string as input and return the same string in reverse order.\n",
            "\n",
            "Step 2: Think of a solution\n",
            "\n",
            "One possible solution is to use a loop that iterates over the characters of the input string and appends them to the end of a new string in reverse order. Here's a step-by-step outline of the solution:\n",
            "\n",
            "Step 3: Write the code\n",
            "\n",
            "def reverse_string(s):\n",
            "    # Create an empty string to store the reversed characters\n",
            "    reversed_string = \"\"\n",
            "    # Iterate over the characters of the input string\n",
            "    for char in s:\n",
            "        # Append the character to the end of the reversed string\n",
            "        reversed_string += char[::-1]\n",
            "    # Return the reversed string\n",
            "    return reversed_string\n",
            "\n",
            "Step 4: Test the code\n",
            "\n",
            "Let's test our function by calling it with a few different input strings:\n",
            "\n",
            "print(reverse_string(\"hello\")) # Output: \"lohel\"\n",
            "print(reverse_string(\"world\")) # Output: \"dlrow\"\n",
            "print(reverse_string(\"level\")) # Output: \"elvel\"\n",
            "\n",
            "As you can see, our function is working correctly and reversing the input strings as expected.\n",
            "\n",
            "Step 5: Reflect on the solution\n",
            "\n",
            "In this solution, we used a loop to iterate over the characters of the input string and append them to a new string in reverse order. This approach works because in Python, strings are immutable, so we can safely modify a copy of the input string without affecting the original.\n",
            "\n",
            "Step 6: Generalize the solution\n",
            "\n",
            "This solution can be generalized to work for any string input, regardless of its length. We can also optimize the code by using a more efficient loop\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Test the model\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fux9om_c4-"
      },
      "source": [
        "# Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hxQ_Ero2IJe"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\n",
        "# num_new_tokens = 500  # change to the number of new tokens you want to generate\n",
        "\n",
        "# # Count the number of tokens in the prompt\n",
        "# num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
        "\n",
        "# # Calculate the maximum length for the generation\n",
        "# max_length = num_prompt_tokens + num_new_tokens\n",
        "\n",
        "# gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n",
        "# result = gen(prompt)\n",
        "# print(result[0]['generated_text'].replace(prompt, ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko6UkINu_qSx"
      },
      "source": [
        "#Merge the model and store in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgKCL7fTyp9u"
      },
      "outputs": [],
      "source": [
        "# # Merge and save the fine-tuned model\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n",
        "\n",
        "# # Reload model in FP16 and merge it with LoRA weights\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     low_cpu_mem_usage=True,\n",
        "#     return_dict=True,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     device_map=device_map,\n",
        "# )\n",
        "# model = PeftModel.from_pretrained(base_model, new_model)\n",
        "# model = model.merge_and_unload()\n",
        "\n",
        "# # Reload tokenizer to save it\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.padding_side = \"right\"\n",
        "\n",
        "# # Save the merged model\n",
        "# model.save_pretrained(model_path)\n",
        "# tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-dFdE5zWGO"
      },
      "source": [
        "# Load a fine-tuned model from Drive and run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg6nHPsLzMw-"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBK2aE2KzZ05"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# prompt = \"What is 2 + 2?\"  # change to your desired prompt\n",
        "# gen = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "# result = gen(prompt)\n",
        "# print(result[0]['generated_text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "6a72b70942e5d8a71626ab00c553ae8717673b1d07904673137f010eb6b41673"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
