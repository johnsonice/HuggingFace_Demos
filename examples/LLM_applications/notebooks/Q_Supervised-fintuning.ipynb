{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLORA Supervised Finetuning process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - references:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0,'../')\n",
    "sys.path.insert(0,'../../../libs')\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "## project specific modules\n",
    "import config\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and explore datasets\n",
    "- in this example we will just use Dolly from Databricks. \n",
    "- for domain specific fintuning, you should use your own data or some kind of mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n",
      "dataset size: 15011\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\",cache_dir=config.cache_dir)\n",
    "print(dataset[0])\n",
    "print(f\"dataset size: {len(dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we are using llama2 chat model, we should follow llama2 prompt template for our training data\n",
    "- if we are using other pretrianing model, we should use the corespond training prompt template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "<</SYS>>\n",
      "\n",
      "When did Virgin Australia start operating? ###Input:Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney. [/INST] Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    transform them into llama chat style template \n",
    "    \"\"\"\n",
    "    system_message = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'\n",
    "    prompt = '{} ###Input:{}'.format(sample['instruction'],sample['context'])\n",
    "    response  = sample['response']\n",
    "    sample['text'] = f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response\n",
    "    return sample\n",
    "\n",
    "print(create_prompt_formats(dataset[0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Classify each of these car manufacturers as either French, German, American, Japanese, or other: Lexus, Hyundai, Alfa Romeo, VW, Honda, GM, Citroen', 'context': '', 'response': 'French: Citroen\\nGerman: VW\\nAmerican: GM\\nJapanese: Lexus, Honda\\nother: Hyundai, Alfa Romeo', 'category': 'classification', 'text': '[INST] <<SYS>>\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n<</SYS>>\\n\\nClassify each of these car manufacturers as either French, German, American, Japanese, or other: Lexus, Hyundai, Alfa Romeo, VW, Honda, GM, Citroen ###Input: [/INST] French: Citroen\\nGerman: VW\\nAmerican: GM\\nJapanese: Lexus, Honda\\nother: Hyundai, Alfa Romeo'}\n"
     ]
    }
   ],
   "source": [
    "## format and split data \n",
    "split_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "split_dataset = split_dataset.map( create_prompt_formats) # ,remove_columns=['instruction','context','response']); use the sft trainer, remove is not necessary\n",
    "train_dataset, test_dataset = split_dataset['train'],split_dataset['test']\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model and set bnb config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    # bitsandbytes parameters\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit= True, # Activate 4-bit precision base model loading\n",
    "        bnb_4bit_quant_type=\"nf4\", # Quantization type (fp4 or nf4)\n",
    "        bnb_4bit_compute_dtype= \"float16\", # Compute dtype for 4-bit base models\n",
    "        bnb_4bit_use_double_quant= False, # Activate nested quantization for 4-bit base models (double quantization)\n",
    "    )\n",
    "    compute_dtype = getattr(torch, 'float16') # Load tokenizer and model with QLoRA configuration\n",
    "\n",
    "    # Check GPU compatibility with bfloat16\n",
    "    if compute_dtype == torch.float16 and bnb_config.load_in_4bit:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "            print(\"=\" * 80)\n",
    "    \n",
    "    return bnb_config\n",
    "\n",
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    #max_memory = f'{40960}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        cache_dir= config.cache_dir\n",
    "        #max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    model.config.use_cache = False    ## not exactly sure why need to set this one\n",
    "    model.config.pretraining_tp = 1   ## not exactly sure why need to set this one\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,cache_dir= config.cache_dir)\n",
    "    \n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load LoRA configuration and set training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        #target_modules=modules,   ## targeted lora modules; if want to specify see https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=os.path.join(config.data_folder3,\"results\",\"llama-2-7b-dolly-custom\"),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1, # Number of training steps (overrides num_train_epochs)\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True, # Group sequences into batches with same length, Saves memory and speeds up training considerably\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    #report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cdb0eb1bae45c7b95b712d63f2cb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "bnb_config = create_bnb_config()\n",
    "model,tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu.huang/anaconda3/envs/llm/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,  # If none is passed, the trainer will retrieve that value from the tokenizer. \n",
    "                          # Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. \n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,        # already used groupping \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='845' max='845' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [845/845 58:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.098100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.model.save_pretrained(os.path.join(config.data_folder3,'results_final','llama-2-7b-dolly-custom'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a72b70942e5d8a71626ab00c553ae8717673b1d07904673137f010eb6b41673"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
