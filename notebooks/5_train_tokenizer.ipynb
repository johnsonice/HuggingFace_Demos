{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468682db",
   "metadata": {},
   "source": [
    "## Train a new tokenizer based on existing models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d16214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e1569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset code_search_net (/home/chengyu/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40661a51e22b423a96972e17374c0594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b452fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fetch_genome(genome_id):\n",
      "    '''Acquire a genome from Entrez\n",
      "\n",
      "    '''\n",
      "    # TODO: Can strandedness by found in fetched genome attributes?\n",
      "    # TODO: skip read/write step?\n",
      "    # Using a dummy email for now - does this violate NCBI guidelines?\n",
      "    email = 'loremipsum@gmail.com'\n",
      "    Entrez.email = email\n",
      "\n",
      "    print 'Downloading Genome...'\n",
      "    handle = Entrez.efetch(db='nucleotide', id=str(genome_id), rettype='gb',\n",
      "                           retmode='text')\n",
      "    print 'Genome Downloaded...'\n",
      "    tmpfile = os.path.join(mkdtemp(), 'tmp.gb')\n",
      "    with open(tmpfile, 'w') as f:\n",
      "        f.write(handle.read())\n",
      "    genome = coral.seqio.read_dna(tmpfile)\n",
      "\n",
      "    return genome\n"
     ]
    }
   ],
   "source": [
    "## print one raw data to inspect\n",
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cd999",
   "metadata": {},
   "source": [
    "### 1) We will have to transform the dataset into an iterator of lists of texts \n",
    "- Using a Python generator, we can avoid Python loading anything into memory until it’s actually necessary. To create such a generator, you just to need to replace the brackets with parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07dbfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make it a generator \n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8f959",
   "metadata": {},
   "source": [
    "### 2) Train a new Tokenizer \n",
    "- Even though we are going to train a new tokenizer, it’s a good idea to do this to avoid starting entirely from scratch. This way, we won’t have to specify anything about the tokenization algorithm or the special tokens we want to use; our new tokenizer will be exactly the same as GPT-2, and the only thing that will change is the vocabulary, which will be determined by the training on our corpus.\n",
    "- Note that AutoTokenizer.train_new_from_iterator() only works if the tokenizer you are using is a “fast” tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7dff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for example, let's try to use gpt2 tokenizer as base \n",
    "# this can take a bit of time \n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953d19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'ĠLinear', 'Layer', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġdef', 'Ġ__', 'init', '__', '(', 'self', ',', 'Ġinput', '_', 'size', ',', 'Ġoutput', '_', 'size', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'rand', 'n', '(', 'input', '_', 'size', ',', 'Ġoutput', '_', 'size', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġself', '.', 'b', 'ias', 'Ġ=', 'Ġtorch', '.', 'zer', 'os', '(', 'output', '_', 'size', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġdef', 'Ġ__', 'call', '__', '(', 'self', ',', 'Ġx', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'b', 'ias', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ']\n",
      "['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',', 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']\n"
     ]
    }
   ],
   "source": [
    "## now we can see an example of the results \n",
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "print(old_tokenizer.tokenize(example))\n",
    "print(tokenizer.tokenize(example))\n",
    "\n",
    "## we can see several originally seperated words are now groupped together "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ef33f",
   "metadata": {},
   "source": [
    "### 4) Save trained tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b24e7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/tokenizer_config.json',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/special_tokens_map.json',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/vocab.json',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/merges.txt',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/added_tokens.json',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdir = os.path.join(config.data_folder,'new_tokenizer','code-search-net-tokenizer')\n",
    "tokenizer.save_pretrained(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa737379",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(outdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3",
   "language": "python",
   "name": "gpt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
