{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468682db",
   "metadata": {},
   "source": [
    "## Train a new tokenizer based on existing models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d16214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e1569c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4f86d423f34d04afc54724874247b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125af32dd301492ebfbfdd690c0b2739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset code_search_net/python (download: 897.32 MiB, generated: 1.62 GiB, post-processed: Unknown size, total: 2.49 GiB) to /Users/huang/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f50bda716d4b37a96353148046e612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a691ac3dd884e68af227cdd5dd85f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ade575046b84f91b8e25f1ccfd829f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e2dce5e48a4d95bcde09bd27bc7078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset code_search_net downloaded and prepared to /Users/huang/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99276bdbf2f4c21aaaa0b0ca1aec085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b452fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def updater():\n",
      "    \"\"\"Update the current installation.\n",
      "\n",
      "    git clones the latest version and merges it with the current directory.\n",
      "    \"\"\"\n",
      "    print('%s Checking for updates' % run)\n",
      "    # Changes must be separated by ;\n",
      "    changes = '''major bug fixes;removed ninja mode;dropped python < 3.2 support;fixed unicode output;proxy support;more intels'''\n",
      "    latest_commit = requester('https://raw.githubusercontent.com/s0md3v/Photon/master/core/updater.py', host='raw.githubusercontent.com')\n",
      "    # Just a hack to see if a new version is available\n",
      "    if changes not in latest_commit:\n",
      "        changelog = re.search(r\"changes = '''(.*?)'''\", latest_commit)\n",
      "        # Splitting the changes to form a list\n",
      "        changelog = changelog.group(1).split(';')\n",
      "        print('%s A new version of Photon is available.' % good)\n",
      "        print('%s Changes:' % info)\n",
      "        for change in changelog: # print changes\n",
      "            print('%s>%s %s' % (green, end, change))\n",
      "\n",
      "        current_path = os.getcwd().split('/') # if you know it, you know it\n",
      "        folder = current_path[-1] # current directory name\n",
      "        path = '/'.join(current_path) # current directory path\n",
      "        choice = input('%s Would you like to update? [Y/n] ' % que).lower()\n",
      "\n",
      "        if choice != 'n':\n",
      "            print('%s Updating Photon' % run)\n",
      "            os.system('git clone --quiet https://github.com/s0md3v/Photon %s'\n",
      "                      % (folder))\n",
      "            os.system('cp -r %s/%s/* %s && rm -r %s/%s/ 2>/dev/null'\n",
      "                      % (path, folder, path, path, folder))\n",
      "            print('%s Update successful!' % good)\n",
      "    else:\n",
      "        print('%s Photon is up to date!' % good)\n"
     ]
    }
   ],
   "source": [
    "## print one raw data to inspect\n",
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cd999",
   "metadata": {},
   "source": [
    "### 1) We will have to transform the dataset into an iterator of lists of texts \n",
    "- Using a Python generator, we can avoid Python loading anything into memory until it’s actually necessary. To create such a generator, you just to need to replace the brackets with parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07dbfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make it a generator \n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8f959",
   "metadata": {},
   "source": [
    "### 2) Train a new Tokenizer \n",
    "- Even though we are going to train a new tokenizer, it’s a good idea to do this to avoid starting entirely from scratch. This way, we won’t have to specify anything about the tokenization algorithm or the special tokens we want to use; our new tokenizer will be exactly the same as GPT-2, and the only thing that will change is the vocabulary, which will be determined by the training on our corpus.\n",
    "- Note that AutoTokenizer.train_new_from_iterator() only works if the tokenizer you are using is a “fast” tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7dff69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b09e0be38aa4d2a83065261e99f445e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d510f0ab45424ea581dff38c3aa99314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2db0bba0df41dbba69dfd7300274de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43767a0501c54bf88ab568a51ce85a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for example, let's try to use gpt2 tokenizer as base \n",
    "# this can take a bit of time \n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953d19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'ĠLinear', 'Layer', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġdef', 'Ġ__', 'init', '__', '(', 'self', ',', 'Ġinput', '_', 'size', ',', 'Ġoutput', '_', 'size', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'rand', 'n', '(', 'input', '_', 'size', ',', 'Ġoutput', '_', 'size', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġself', '.', 'b', 'ias', 'Ġ=', 'Ġtorch', '.', 'zer', 'os', '(', 'output', '_', 'size', ')', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġdef', 'Ġ__', 'call', '__', '(', 'self', ',', 'Ġx', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'b', 'ias', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ']\n",
      "['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',', 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_', 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']\n"
     ]
    }
   ],
   "source": [
    "## now we can see an example of the results \n",
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "print(old_tokenizer.tokenize(example))\n",
    "print(tokenizer.tokenize(example))\n",
    "\n",
    "## we can see several originally seperated words are now groupped together "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddefca1",
   "metadata": {},
   "source": [
    "#### We can compare the vocabular and add additional to original tokenizer follwoing: https://github.com/georgian-io/Transformers-Domain-Adaptation/blob/master/notebooks/GuideToTransformersDomainAdaptation.ipynb\n",
    "- but with out using their packeage (They have an outdated tokenizer version)\n",
    "- also in our MLM pretraining notebook, there is a vocabulary expansing step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e801cf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32161\n"
     ]
    }
   ],
   "source": [
    "new_key = []\n",
    "for k in tokenizer.vocab.keys():\n",
    "    if old_tokenizer.vocab.get(k) is None:\n",
    "        new_key.append(k)\n",
    "print(len(new_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ef33f",
   "metadata": {},
   "source": [
    "### 4) Save trained tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b24e7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/tokenizer_config.json',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/special_tokens_map.json',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/vocab.json',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/merges.txt',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/added_tokens.json',\n",
       " '/media/chengyu/Elements1/HuggingFace/new_tokenizer/code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdir = os.path.join(config.data_folder,'new_tokenizer','code-search-net-tokenizer')\n",
    "tokenizer.save_pretrained(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa737379",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(outdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3",
   "language": "python",
   "name": "gpt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
