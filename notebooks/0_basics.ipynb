{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52786acb-4fd7-42ef-a569-b85a3495744c",
   "metadata": {},
   "source": [
    "# Basic usage of high level APIs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1639d0ad-5c26-4f56-8942-c8c53e3ef700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42418a-3908-48ec-bc37-71036687a796",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic sentiment analysis - using pretrained models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d22220-de74-4a95-bf07-69029468c16a",
   "metadata": {},
   "source": [
    "- #### the pipeline will downlaod a default pretrained model (if not specified it is distilled bert base), fineturned on SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e20e0ec-32a1-4adf-8202-2be1af8fa564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98416002-06bc-4a4d-82bc-6ccf45c1ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\", \n",
    "                      \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542df01a-e9df-4488-bc08-b7dda21594e9",
   "metadata": {},
   "source": [
    "### Use specific model and model tokenizers in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200e0b09-7638-4905-b10c-95ebf34d084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "666ce9a4-6287-4036-8dac-19c16f9ceb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed1ad4d8-7ab3-4961-8d11-a53d9cf404da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272651791572571}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48b4cc-7a84-4e33-b32a-bba1d2bf0d42",
   "metadata": {},
   "source": [
    "- now we can tokenize a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833c4ea1-77f4-4891-8f18-1f16b0e1a308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e6e9a-9c77-4cc3-bd20-e595f15666f4",
   "metadata": {},
   "source": [
    "- we can also tokenize a batch with padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb7c0cbd-f152-4cdc-b301-bea165182970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103,   100,\n",
      "         58263, 13299,   119,   102],\n",
      "        [  101, 11312, 18763, 10855, 11530,   112,   162, 39487, 10197,   119,\n",
      "           102,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(pt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e5734-7ec6-4984-b93a-c21dd762cc4e",
   "metadata": {},
   "source": [
    "- #### we can now use the mode and tokeinzed data for prediction; The model outputs the final activations in the logits attribute. Apply the softmax function to the logits to retrieve the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa8abc90-2899-4c36-b292-b861749a4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "961df90d-a9da-46b2-b5a5-d5d6b2ffaee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
      "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pt_outputs = model(**pt_batch)\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466d41f-1b13-4cd9-a771-17f7710ff031",
   "metadata": {},
   "source": [
    "- #### Once finished fineturning your model, you can save model weights locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba63a10f-6025-4da2-b396-047fc730051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"/Volumes/T7/Data/Pretrained_models/example\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9460c16-155f-4d33-8599-90e625cbbf30",
   "metadata": {},
   "source": [
    "- #### You can now reload from saved path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4028667-42b6-4b8b-96f4-086e38d01fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11caea20-9471-4218-b648-6be6c53d71d1",
   "metadata": {},
   "source": [
    "#### Models are donloaded in a local cach folder, you can config this following : https://huggingface.co/docs/transformers/v4.17.0/en/installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72baf4f-cc2e-4f5f-bce1-04ee27a562cb",
   "metadata": {},
   "source": [
    "#### You can also manually download pretrained modles weights locally : https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "112eec72-de44-43d4-ba0c-e6682e670ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet_path = \"/Volumes/T7/Data/Pretrained_models/bertweet-base-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bertweet_path)\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(bertweet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2af9f6b6-f7de-4f3c-988d-a16dfe2bb3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   134,    41,   249,   225,     9,   258,    14,     6,     3,\n",
      "         27615, 42593, 10028,     4,     2],\n",
      "        [    0,   134,   240,    14, 19933,   253,   987,     4,     2,     1,\n",
      "             1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \n",
    "     \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(pt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c065efd-9cf6-4808-95f3-45613911b3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3",
   "language": "python",
   "name": "gpt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
