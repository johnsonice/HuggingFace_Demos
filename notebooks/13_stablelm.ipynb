{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT alternatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  5 21:10:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   49C    P8    26W / 350W |   1151MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 30%   44C    P8    14W / 350W |    773MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1402      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1546      G   /usr/bin/gnome-shell                8MiB |\n",
      "|    0   N/A  N/A   3216083      C   ...da3/envs/sbert/bin/python     1129MiB |\n",
      "|    1   N/A  N/A      1402      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   3216083      C   ...da3/envs/sbert/bin/python      765MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. StableLM\n",
    "- https://github.com/Stability-AI/StableLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:48<00:00, 42.07s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 111/111 [00:00<00:00, 125kB/s]\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/data/hf_cache'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-tuned-alpha-7b\",cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-tuned-alpha-7b\",cache_dir=cache_dir)\n",
    "model.half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = set([50278, 50279, 50277, 1, 0])\n",
    "        return input_ids[0][-1] in stop_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "user_prompt = 'Please summarize what a large language model can do in ten bullet points.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Provide natural language processing capabilities for applications such as chatbots, virtual assistants, and customer service\n",
      "* Improved text generation with better grammar and style control\n",
      "* Enhanced machine translation with improved accuracy and fluency\n",
      "* Better understanding of context and relationships between words\n",
      "* More accurate sentiment analysis and classification of text data\n",
      "* Increased ability to generate coherent and contextually relevant text\n",
      "* Support for knowledge management and organization of large amounts of text data\n",
      "* Ability to perform tasks such as text summarization, paraphrasing, and question answering\n",
      "* Better integration with other tools and systems\n",
      "* Potential for increased use in legal and educational contexts\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"{system_prompt}<|USER|>{user_prompt}<|ASSISTANT|>\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=512,\n",
    "  temperature=0.1,\n",
    "  do_sample=True,\n",
    "  stopping_criteria=StoppingCriteriaList([StopOnTokens()]),\n",
    "  repetition_penalty=1.1\n",
    ")\n",
    "res = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "print(res.replace(system_prompt.strip('<|SYSTEM|>'),'').replace(user_prompt,''))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. StableVicunna\n",
    "- need more recent transformer library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- !pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "- !pip install -q datasets loralib sentencepiece \n",
    "- !pip -q install bitsandbytes accelerate\n",
    "- when encounter bitsandbytes errors see [link](https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1474056975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/chengyu.huang/anaconda3/envs/sbert did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/chengyu.huang/anaconda3/envs/sbert/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:35<00:00, 11.90s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 137/137 [00:00<00:00, 48.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "## load heBloke/stable-vicuna-13B-HF ## some user already converted it to HF format \n",
    "cache_dir = '/data/hf_cache'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\",cache_dir=cache_dir)\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"TheBloke/stable-vicuna-13B-HF\",\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',    ## here, i am in a cpu only environment, so commneted out this \n",
    "    cache_dir=cache_dir,\n",
    "    #torch_dtype  = torch.float16   ## here, i am in a cpu only environment, so load in 16bit  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt & response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: What is the meaning of life? \n",
      "### Assistant:\n",
      "The capital city of England is London.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "human_prompt = 'What is the meaning of life?'\n",
    "\n",
    "def get_prompt(human_prompt):\n",
    "    prompt_template=f\"### Human: {human_prompt} \\n### Assistant:\"\n",
    "    return prompt_template\n",
    "\n",
    "print(get_prompt('What is the meaning of life?'))\n",
    "\n",
    "def remove_human_text(text):\n",
    "    return text.split('### Human:', 1)[0]\n",
    "\n",
    "def parse_text(data):\n",
    "    for item in data:\n",
    "        text = item['generated_text']\n",
    "        assistant_text_index = text.find('### Assistant:')\n",
    "        if assistant_text_index != -1:\n",
    "            assistant_text = text[assistant_text_index+len('### Assistant:'):].strip()\n",
    "            assistant_text = remove_human_text(assistant_text)\n",
    "            wrapped_text = textwrap.fill(assistant_text, width=100)\n",
    "            print(wrapped_text)\n",
    "\n",
    "data = [{'generated_text': '### Human: What is the capital of England? \\n### Assistant: The capital city of England is London.'}]\n",
    "parse_text(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run some test examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas, alpacas, and vicuñas are all members of the camelid family. However, there are some\n",
      "differences between them:  1. Appearance: Llamas have a stockier build than alpacas and vicuñas,\n",
      "which are more slender. Llamas also have longer fur that can be used for clothing or other products.\n",
      "2. Size: Llamas are generally larger than both alpacas and vicuñas. They can weigh up to 500 pounds\n",
      "(227 kg), while alpacas typically weigh around 150-200 pounds (68-91 kg) and vicuñas around 130-140\n",
      "pounds (59-63 kg).  3. Temperament: Llamas tend to be calmer and less skittish than alpacas and\n",
      "vicuñas. This makes them easier to handle and train for tasks like packing or pulling carts.  4.\n",
      "Uses: Llamas are often used as pack animals in South America, while alpacas and vicuñas are valued\n",
      "for their soft, luxurious fleece, which is used to make high-end clothing and accessories.  Overall,\n",
      "each species has its own unique characteristics and uses within their respective regions.\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "raw_output = pipe(get_prompt('What are the difference between Llamas, Alpacas and Vicunas?'))\n",
    "parse_text(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several factors that can drive inflation, including changes in supply and demand for goods\n",
      "and services, fluctuations in currency exchange rates, and government policies such as taxes or\n",
      "subsidies. In recent years, rising oil prices have also been a major driver of inflation due to\n",
      "their impact on transportation costs and other industries. Additionally, economic growth and job\n",
      "creation can lead to increased consumer spending, which can further fuel inflation. However, it is\n",
      "important to note that there are many complex factors at play when it comes to inflation, and its\n",
      "causes can vary depending on the specific circumstances.\n"
     ]
    }
   ],
   "source": [
    "raw_output = pipe(get_prompt('What drives inflation in the past year?'))\n",
    "parse_text(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a macro-economic setting, the current account balance refers to the difference between a\n",
      "country's imports and exports of goods and services. It measures the net flow of foreign currency\n",
      "into or out of a country's economy. A positive current account balance indicates that a country has\n",
      "more assets than liabilities abroad, while a negative current account balance suggests that a\n",
      "country owes more money to foreign creditors than it receives from them.\n"
     ]
    }
   ],
   "source": [
    "raw_output = pipe(get_prompt('What is current account balance in a macro-economic setting?'))\n",
    "parse_text(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are ten things that a large language model can do: 1. Generate text based on prompts or inputs.\n",
      "2. Answer questions and provide information about various topics. 3. Translate between languages. 4.\n",
      "Summarize long pieces of text. 5. Write creative stories, poems, and other forms of literature. 6.\n",
      "Create natural-sounding speech from written text. 7. Classify objects and images using computer\n",
      "vision techniques. 8. Predict the next word or sentence given a context. 9. Play games like chess or\n",
      "Go by analyzing board positions. 10. Perform data analysis tasks such as clustering and anomaly\n",
      "detection.\n"
     ]
    }
   ],
   "source": [
    "raw_output = pipe(get_prompt('Please summarize what a large language model can do in ten bullet points.'))\n",
    "parse_text(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relationship between interest rates and inflation is complex, but there are a few key factors to\n",
      "consider. When the Federal Reserve raises interest rates, it can have both positive and negative\n",
      "effects on inflation. On one hand, higher interest rates make borrowing more expensive for consumers\n",
      "and businesses, which can lead to less spending and investment in the economy. This decrease in\n",
      "economic activity can slow down overall growth and potentially lead to lower inflation over time.\n",
      "However, if the Fed raises interest rates too much or too quickly, it can also cause a recession,\n",
      "which would increase unemployment and reduce wages, leading to higher inflation in the long run.  On\n",
      "the other hand, when the Fed lowers interest rates, it can stimulate economic activity by making\n",
      "borrowing cheaper and encouraging people to spend and invest more. This increased demand for goods\n",
      "and services can drive up prices and lead to higher inflation. Additionally, lower interest rates\n",
      "can encourage banks to lend more money, which can fuel asset bubbles and lead to speculative\n",
      "behavior that contributes to inflation.  Overall, the relationship between interest rates and\n",
      "inflation is not straightforward, as there are many factors at play. While raising interest rates\n",
      "may help curb inflation in the short term, it can also have negative consequences for the economy in\n",
      "the long run. Similarly, lowering interest rates can boost economic growth, but it can also\n",
      "contribute to inflation if not done carefully. Therefore, the Fed must strike a delicate balance\n",
      "between managing inflation and promoting economic growth.\n"
     ]
    }
   ],
   "source": [
    "raw_output = pipe(get_prompt('Please write an essay explaining how intrest rate affect inflation.'))\n",
    "parse_text(raw_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OpenAssistant 30b\n",
    "- we are using HF version converted by Bloke [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"tokenizer.model\";: 100%|██████████| 500k/500k [00:00<00:00, 9.66MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 388/388 [00:00<00:00, 132kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 477/477 [00:00<00:00, 504kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 715/715 [00:00<00:00, 799kB/s]\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 568/568 [00:00<00:00, 237kB/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 50.1k/50.1k [00:00<00:00, 4.35MB/s]\n",
      "Downloading (…)00001-of-00007.bin\";: 100%|██████████| 9.82G/9.82G [02:11<00:00, 74.7MB/s]\n",
      "Downloading (…)00002-of-00007.bin\";: 100%|██████████| 9.96G/9.96G [02:17<00:00, 72.3MB/s]\n",
      "Downloading (…)00003-of-00007.bin\";: 100%|██████████| 9.90G/9.90G [02:15<00:00, 72.9MB/s]\n",
      "Downloading (…)00004-of-00007.bin\";: 100%|██████████| 9.87G/9.87G [02:23<00:00, 68.8MB/s]\n",
      "Downloading (…)00005-of-00007.bin\";: 100%|██████████| 9.87G/9.87G [02:21<00:00, 69.8MB/s]\n",
      "Downloading (…)00006-of-00007.bin\";: 100%|██████████| 9.96G/9.96G [02:17<00:00, 72.5MB/s]\n",
      "Downloading (…)00007-of-00007.bin\";: 100%|██████████| 5.69G/5.69G [01:24<00:00, 67.3MB/s]\n",
      "Downloading shards: 100%|██████████| 7/7 [15:13<00:00, 130.57s/it]\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/chengyu.huang/anaconda3/envs/sbert did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/chengyu.huang/anaconda3/envs/sbert/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [06:15<00:00, 53.65s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 137/137 [00:00<00:00, 50.6kB/s]\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/data/hf_cache'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"TheBloke/OpenAssistant-SFT-7-Llama-30B-HF\",cache_dir=cache_dir)\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"TheBloke/OpenAssistant-SFT-7-Llama-30B-HF\",\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',    ## here, i am in a cpu only environment, so commneted out this \n",
    "    cache_dir=cache_dir,\n",
    "    #torch_dtype  = torch.float16   ## here, i am in a cpu only environment, so load in 16bit  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output = pipe('Please write an essay explaining how intrest rate affect inflation.<|assistant|>:')\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '<|prompter|>Please write an essay explaining how intrest rate affect inflation.<|assistant|>:\\n\\nInflation is the increase in prices of goods and services over a period of time, while interest rates are the cost of borrowing money from banks or other financial institutions. The relationship between these two economic indicators is complex and multifaceted, but there are several key ways that interest rates can impact inflation:\\n\\n1. Monetary policy: Central banks use monetary policy to control inflation by adjusting interest rates. When inflation rises above a certain level, central banks may raise interest rates to reduce demand for credit and slow down economic growth. This can help bring inflation back under control. Conversely, when inflation falls below target levels, central banks may lower interest rates to stimulate economic activity and boost inflation.\\n2. Consumer spending: Interest rates also influence consumer behavior and spending patterns. Higher interest rates make it more expensive to borrow money, which can discourage consumers from making large purchases like homes or cars. This can lead to reduced demand for goods and services, which can help keep inflation in check. On the other hand, low interest rates can encourage consumers to take on debt and spend more freely, which can drive up inflation.\\n3. Investment: Interest rates can also affect investment decisions. When interest rates are high, it becomes less attractive for businesses to invest in new projects or expand their operations. This can limit supply and put upward pressure on prices, leading to higher inflation. Conversely, when interest rates are low, businesses may be more willing to invest and expand, which can help keep inflation in check.\\n4. Exchange rates: Interest rates can also have an impact on exchange rates, as investors often compare the returns they can earn on different currencies. If interest rates in one country are significantly higher than those in another, this can lead to capital flows into the first country, driving up its currency value and potentially reducing inflation. However, if interest rates'}]\n"
     ]
    }
   ],
   "source": [
    "raw_output = pipe('<|prompter|>Please write an essay explaining how intrest rate affect inflation.<|assistant|>:')\n",
    "print(raw_output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cf887a0cd63b1646b72eb9cdb014d6c9733a8a0a6989a432f0f7542bfe6547e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
