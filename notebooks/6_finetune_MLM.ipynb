{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9698b0",
   "metadata": {},
   "source": [
    "## Fine-tuning a masked language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454afdc1",
   "metadata": {},
   "source": [
    "- For this exercise, we will just DistilBERT as an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a032a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45e883ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from transformers import default_data_collator\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cfc3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949e9e2",
   "metadata": {},
   "source": [
    "- Basic explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d2317f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> DistilBERT number of parameters: 67M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809e493",
   "metadata": {},
   "source": [
    "- Try the masked language prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148553bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a great [MASK].\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45c142",
   "metadata": {},
   "source": [
    "### Load dataset for finetuning language model \n",
    "- here we will sue imdb for fintuning purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fd3232c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b36bd600f2e435a8944b2aa774c3e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6c9e1e496a4d069c407c3b1b120a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/chengyu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa9952c858f4379a2bbb2c7ac348453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995064cb",
   "metadata": {},
   "source": [
    "- some basic data exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba88c6b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/chengyu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-8a9e43a6ac4acdff.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...'\n",
      "'>>> Label: 1'\n",
      "\n",
      "'>>> Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.'\n",
      "'>>> Label: 1'\n"
     ]
    }
   ],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(2))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48de09",
   "metadata": {},
   "source": [
    "- if your deomain data has a lot domain specific terms, it is a good idea to do an vocabulary expansion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a996115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_domain_adaptation import VocabAugmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e24e8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## original vocabulary size \n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "534143ed",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert <tokenizers.trainers.WordPieceTrainer object at 0x7ffa12816e90> to Sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28326/1008139258.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Obtain new domain-specific terminology based on the fine-tuning corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnew_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_new_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gpt3/lib/python3.8/site-packages/transformers_domain_adaptation/vocab_augmentor.py\u001b[0m in \u001b[0;36mget_new_tokens\u001b[0;34m(self, training_corpus)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Train new tokenizer on `ft_corpus`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_training_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_tmpfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrust_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Include unknown token to vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert <tokenizers.trainers.WordPieceTrainer object at 0x7ffa12816e90> to Sequence"
     ]
    }
   ],
   "source": [
    "## set a new vocab size limit  \n",
    "target_vocab_size = 31_000  # len(tokenizer) == 30_522\n",
    "augmentor = VocabAugmentor(\n",
    "    tokenizer=tokenizer,   ## the domain_adaptation package require tokenizer to be a older version. may need another environment\n",
    "    cased=False,    # change this based on your tokenizer\n",
    "    target_vocab_size=target_vocab_size\n",
    ")\n",
    "\n",
    "# Obtain new domain-specific terminology based on the fine-tuning corpus\n",
    "new_tokens = augmentor.get_new_tokens(imdb_dataset[\"train\"]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6d3f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n",
      "['/><', '/>', '.<', '),', ').', '--', '\",', '\".', '.\"', ',\"', '\\x96', '..', ').<', '!<', '....', '...<', '.)', '!!', 'dialog', \"',\", '?<', '!\"', 'believable', 'imdb', \"'.\", '!!!', 'cheesy', '/>\"', 'nudity', '!)', 'spoilers', '?\"', '(\"', '\")', 'slasher', '?)', '\".<', 'laughable', 'spoiler', 'likable', 'mediocre', 'cliché', 'funniest', 'flicks', \".'\", 'thats', '.,', 'cgi', 'redeeming', 'wonderfully', 'keaton', 'watchable', 'clichés', '.....', 'pretentious', ':<', '.\"<', '\\x97', '**', '/>-', 'unfunny', 'gags', ');', 'corny', '***', '****', 'cant', 'brilliantly', '!).', 'gratuitous', 'crappy', 'gory', 'underrated', '??', '---', 'contrived', 'unrealistic', 'depressing', 'amateurish', 'tedious', 'lousy', '/>*', \",'\", 'columbo', '):', 'liners', 'forgettable', 'pacino', 'uninteresting', 'atrocious', '???', 'gritty', 'lugosi', 'unconvincing', 'suspenseful', '?!', 'subtitles', ')<', 'melodrama', '!!!!']\n"
     ]
    }
   ],
   "source": [
    "## depends on your use case, maybe you want to do some cleaning on the new vocabularies \n",
    "print(len(new_tokens))\n",
    "print(new_tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca63626",
   "metadata": {},
   "source": [
    "- ### Now very importtant, add new tokens to original tokenizer and resized model embeding layers size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ebd37c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(31000, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed319b9",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf94b1d",
   "metadata": {},
   "source": [
    "- For both auto-regressive and masked language modeling, a common preprocessing step is to concatenate all the examples and then split the whole corpus into chunks of equal size. This is quite different from our usual approach, where we simply tokenize individual examples. Why concatenate everything together? The reason is that individual examples might get truncated if they’re too long, and that would result in losing information that might be useful for the language modeling task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952632b",
   "metadata": {},
   "source": [
    "- So to get started, we’ll first tokenize our corpus as usual, but without setting the truncation=True option in our tokenizer. We’ll also grab the word IDs if they are available ((which they will be if we’re using a fast tokenizer, as described in Chapter 6), as we will need them later on to do whole word masking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40023050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24798672a1e54456aff8688e63563490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ca4d6530e64144b9e4f24d8a31f391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033e68423b314f8f9605e751bcb5379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "        ## get world id for whole word masking \n",
    "    return result\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114514af",
   "metadata": {},
   "source": [
    "- Now group them all together and split the result into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b377ea47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#firt check tokenizer max length\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25c69717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to run our experiments on GPUs like those found on Google Colab, we’ll pick something a bit smaller that can fit in memory:\n",
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c2f2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review 0 length: 197'\n",
      "'>>> Review 1 length: 577'\n",
      "'>>> Review 2 length: 195'\n"
     ]
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec8b3f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 969'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys() # for each key, merge all lists\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e25610d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 73'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2590b27",
   "metadata": {},
   "source": [
    "- for the last chunk, it will be smaller than everything else, we will just drop it \n",
    "- now put everything together in one fucntion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0fe61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column, for MLM, we put labels as inputs ids \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1cbc67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db8dbb64c334af7944526a9c732a198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f1bb7ce7ac4104a2d3695c4c4e0aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13388c1fe8d94a75b131ec183e5a8024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
       "        num_rows: 61364\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
       "        num_rows: 60020\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
       "        num_rows: 123243\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## process our data with grouping function\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d93e65",
   "metadata": {},
   "source": [
    "### Insert [MASK] tokens at random positions in the inpouts; we will use a special data collator so that the masking can be apply on the fly\n",
    "- Transformers comes prepared with a dedicated DataCollatorForLanguageModeling for just this task. We just have to pass it the tokenizer and an mlm_probability argument that specifies what fraction of the tokens to mask. We’ll pick 15%, which is the amount used for BERT and a common choice in the literature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b3d643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae745d83",
   "metadata": {},
   "source": [
    "- let's take a look at what this data collector do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c0f2a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] [MASK] [MASK]well high is a sat comedy. it ran at the same time as some other programs about sch ool [MASK], such as reject teachers \". [MASK] 35 years in [MASK] teaching [MASK] lead me to believe that bromwell [MASK]'s satire is much closer to rea lity than is \" teachers \". the scramble to survive financially, [MASK] insightful studentsesis can see right through their pathetic [MASK]'pomp, the pettiness of parody [MASK] situation [MASK] all remind me of the sch ools i knew and their students. when i saw the episode in which a student repeatedly tried [MASK] burn down the sch ool, i immediately'\n",
      "\n",
      "'>>> recalled.. [MASK]..... at.. woodrow...... [MASK]. a [MASK] line [MASK] inspector : i'm here to sack one of your teachers. student [MASK] [MASK] to [MASK]mwell high. i expect [MASK] many adults of [MASK] age think that bromwell [MASK] is far fetched. what a pity that [MASK] i sn't! [SEP] [CLS] homeles sn ess ( or houseles [MASK] ess as george [MASK]in stated ) has been an issue [MASK] r years but never a plan to help those on [MASK] street that were once cons [MASK]ered [MASK] who did everything from going to sch ool, work, or vote marxist r'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples: \n",
    "    _ = sample.pop(\"word_ids\")  ## data collator does not take word_ids as input\n",
    "    \n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\") ## it will just insert mask token in random places "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d7bc3",
   "metadata": {},
   "source": [
    "- One side effect of random masking is that our evaluation metrics will not be deterministic when using the Trainer, since we use the same data collator for the training and test sets. We’ll see later, when we look at fine-tuning with 🤗 Accelerate, how we can use the flexibility of a custom evaluation loop to freeze the randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cd099c",
   "metadata": {},
   "source": [
    "### Whold word Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b03b3",
   "metadata": {},
   "source": [
    "- When training models for masked language modeling, one technique that can be used is to mask whole words together, not just individual tokens. This approach is called whole word masking. If we want to use whole word masking, we will need to build a data collator ourselves. A data collator is just a function that takes a list of samples and converts them into a batch, so let’s do this now! We’ll use the word IDs computed earlier to make a map between word indices and the corresponding tokens, then randomly decide which words to mask and apply that mask on the inputs. Note that the labels are all -100 except for the ones corresponding to mask words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c05a3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a52436ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = lm_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76fc718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features: \n",
    "        word_ids = feature.pop(\"word_ids\")  ## data collator does not take word_ids\n",
    " \n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)  ## create a whold word to token ids map\n",
    "\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),)) ## create a 0 , 1 list with x prob to be 1\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels) \n",
    "        for word_id in np.where(mask)[0]: ## loop through all masked idx \n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]: ## use mapping to get token indexes \n",
    "                new_labels[idx] = labels[idx]  ## in lables, masked token will be actual token index\n",
    "                input_ids[idx] = tokenizer.mask_token_id   ## masked tokens will be replaced with mask token id\n",
    "    \n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be475a2",
   "metadata": {},
   "source": [
    "- try it on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc907c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] bromwell high is a cartoon comedy. it ran at the same time as some other [MASK] about sch ool [MASK], such as \" teachers \". my [MASK] years in the [MASK] [MASK] lead me to [MASK] [MASK] bromwell high's satire is much closer [MASK] rea [MASK] [MASK] than [MASK] \" teachers \". [MASK] [MASK] to survive financially, the insightful [MASK] [MASK] can see right through [MASK] [MASK] [MASK]'pomp, the pettiness of [MASK] whole situation, all remind me [MASK] the sch ools [MASK] knew and their [MASK] [MASK] when [MASK] saw the episode [MASK] which a [MASK] repeatedly tried to burn down [MASK] sch ool, i immediately'\n",
      "\n",
      "'>>> [MASK]...... [MASK]. at.... [MASK].... high. a [MASK] line : inspector [MASK] i'[MASK] here to sack one of [MASK] teachers. [MASK] : welcome [MASK] bromwell high. i expect [MASK] many adults of my age [MASK] [MASK] [MASK] [MASK] [MASK] high is far fetched [MASK] what [MASK] pity that it i [MASK]'[MASK]! [SEP] [CLS] homeles sn ess ( [MASK] houseles [MASK] [MASK] [MASK] as george carlin [MASK] ) has [MASK] an issue fo r years but never a plan to help those on the street that were once cons idered [MASK] who did everything from going to sch ool [MASK] work, or vote fo [MASK]'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96bb7b",
   "metadata": {},
   "source": [
    "### Now we can strat training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82498eb2",
   "metadata": {},
   "source": [
    "- for demo purpose, we will first downsample our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97f5d8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(0.5 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860fcdc",
   "metadata": {},
   "source": [
    "-Here we tweaked a few of the default options, including logging_steps to ensure we track the training loss with each epoch. We’ve also used fp16=True to enable mixed-precision training, which gives us another boost in speed. By default, the Trainer will remove any columns that are not part of the model’s forward() method. This means that if you’re using the whole word masking collator, you’ll also need to set remove_unused_columns=False to ensure we don’t lose the word_ids column during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cf59904",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "model_out_dir = os.path.join(config.data_folder,'MLM','models',\"{}-finetuned-imdb\".format(model_name))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_out_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs = 10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #push_to_hub=False,  ## just run locally for now \n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79de1b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccb764fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu/anaconda3/envs/gpt3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='80' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchuang16\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/chengyu/Dev/HuggingFace_Demos/notebooks/wandb/run-20220709_161807-2seg703i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chuang16/huggingface/runs/2seg703i\" target=\"_blank\">/media/chengyu/Elements1/HuggingFace/MLM/models/distilbert-base-uncased-finetuned-imdb</a></strong> to <a href=\"https://wandb.ai/chuang16/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.292020797729492,\n",
       " 'eval_runtime': 14.3694,\n",
       " 'eval_samples_per_second': 347.96}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see the original perplexity before training\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "faa7cf0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu/anaconda3/envs/gpt3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chengyu/anaconda3/envs/gpt3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [790/790 12:27, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.036516</td>\n",
       "      <td>12.924100</td>\n",
       "      <td>386.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.271300</td>\n",
       "      <td>2.891255</td>\n",
       "      <td>12.865800</td>\n",
       "      <td>388.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.271300</td>\n",
       "      <td>2.783624</td>\n",
       "      <td>12.811100</td>\n",
       "      <td>390.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.919400</td>\n",
       "      <td>2.738822</td>\n",
       "      <td>13.129400</td>\n",
       "      <td>380.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.919400</td>\n",
       "      <td>2.704603</td>\n",
       "      <td>12.909500</td>\n",
       "      <td>387.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.797900</td>\n",
       "      <td>2.669362</td>\n",
       "      <td>13.031000</td>\n",
       "      <td>383.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.797900</td>\n",
       "      <td>2.646639</td>\n",
       "      <td>13.373800</td>\n",
       "      <td>373.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.752300</td>\n",
       "      <td>2.655278</td>\n",
       "      <td>13.401100</td>\n",
       "      <td>373.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.752300</td>\n",
       "      <td>2.636327</td>\n",
       "      <td>13.148600</td>\n",
       "      <td>380.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.731200</td>\n",
       "      <td>2.632193</td>\n",
       "      <td>13.255300</td>\n",
       "      <td>377.208000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu/anaconda3/envs/gpt3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=790, training_loss=2.8925721663462967, metrics={'train_runtime': 748.1911, 'train_samples_per_second': 1.056, 'total_flos': 5172719001600000, 'epoch': 10.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14750816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3",
   "language": "python",
   "name": "gpt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
