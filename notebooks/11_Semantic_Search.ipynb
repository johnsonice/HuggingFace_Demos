{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139c76b3",
   "metadata": {},
   "source": [
    "## Semantic Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a9669",
   "metadata": {},
   "source": [
    "- ### Easy way - use sentence bert package \n",
    "- when you have a small data \n",
    "- depends on your task, model selction is very important , there is a difference on symetric and asymetic search\n",
    "- https://www.sbert.net/examples/applications/semantic-search/README.html\n",
    "- https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5b46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch,os\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  download an embeding model \n",
    "# all-mpnet-base-v2  is currently the best performing model, but \n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2') ## but mini model also achieve similar resulst, but much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d839252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus with example sentences\n",
    "corpus = ['It may be one of the most familiar words in economics. Inflation has plunged countries into long periods of instability. ',\n",
    "          'Inflation is the rate of increase in prices over a given period of time. Inflation is typically a broad measure, such as the overall increase in prices or the increase in the cost of living in a country. ',\n",
    "          'Consumers’ cost of living depends on the prices of many goods and services and the share of each in the household budget. ',\n",
    "          'First, will borrowing remain cheap for the entire horizon relevant for fiscal planning? Since that horizon seems to be the indefinite future, our answer here would be “no.” ',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]\n",
    "# Query sentences:\n",
    "queries = ['Inflation is here to stay for a while', 'Fiscal policy must be supportive']\n",
    "\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "query_embeddings = embedder.encode(queries, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a998fd4",
   "metadata": {},
   "source": [
    "- use cosine similarity to look for neariest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 2\n",
    "# We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "cos_scores = util.cos_sim(query_embeddings, corpus_embeddings)\n",
    "print('results size: {}'.format(cos_scores.size()))\n",
    "top_results = torch.topk(cos_scores, k=top_k)\n",
    "print('top results indices : {}'.format(top_results.indices))\n",
    "print(corpus[top_results.indices[0][0]])\n",
    "print(corpus[top_results.indices[1][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0922d8",
   "metadata": {},
   "source": [
    "- #### To do it more efficiently \n",
    "- use semetic search and normailze embeding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4bccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we can use GPU if we want \n",
    "#corpus_embeddings = corpus_embeddings.to('cuda') ## for GPU\n",
    "corpus_embeddings = util.normalize_embeddings(corpus_embeddings)\n",
    "\n",
    "#query_embeddings = query_embeddings.to('cuda') ## for GPU\n",
    "query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "hits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.dot_score,top_k=2)\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389fe47",
   "metadata": {},
   "source": [
    "- ### More efficient way for large data set \n",
    "- https://huggingface.co/course/chapter5/6?fw=tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375bdc9",
   "metadata": {},
   "source": [
    "- #### Load a large dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d98ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import hf_hub_url\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data \n",
    "issues_dataset = load_dataset('lewtun/github-issues', split=\"train\")\n",
    "print(issues_dataset)\n",
    "## filter and process data \n",
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "print(issues_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0854f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## further data processing \n",
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]\n",
    "comments_df = df.explode(\"comments\", ignore_index=True) ## turn list in a column into seperate rows \n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde84e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter by lentgh \n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")\n",
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "## let's concatenate all content in one field\n",
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7520e3",
   "metadata": {},
   "source": [
    "- ### Create text embeding\n",
    "- here we are doing asymetric search , will use a QA pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf647a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"  ## prefious we are using 'all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f911357",
   "metadata": {},
   "source": [
    "- set get embeding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5376ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pull the cls token embeding \n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bca8c9",
   "metadata": {},
   "source": [
    "- get embedings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d39a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get embeding and save dataset\n",
    "dataset_cache_dir = os.path.join(config.data_folder,'Semantic_Search_cache','QA')\n",
    "if os.path.exists(dataset_cache_dir):\n",
    "    embeddings_dataset = load_from_disk(dataset_cache_dir)\n",
    "else: \n",
    "    embeddings_dataset = comments_dataset.map(\n",
    "        lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().numpy()[0]}\n",
    "    )\n",
    "    embeddings_dataset.save_to_disk(dataset_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12bec70",
   "metadata": {},
   "source": [
    "- ### Use FAISS for efficient similarity search\n",
    "- refer to https://huggingface.co/course/chapter5/6?fw=tf#using-faiss-for-efficient-similarity-search\n",
    "- another example here : https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search_quora_faiss.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3277832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55c67b44829418ca04b80e8fc6bc3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### not sure if add_faiss_index will automatically normalize it. maybe a good idea to normalize ourself \n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3792efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "\n",
    "question_embedding = get_embeddings([question]).detach().numpy()\n",
    "type(question_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a92ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d018fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3",
   "language": "python",
   "name": "gpt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
