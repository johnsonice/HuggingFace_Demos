{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5fea49-0548-4811-a6d3-015f5e287c38",
   "metadata": {},
   "source": [
    "### Tokenize and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca53ada0-7a0e-4e0c-9b87-111e7b80ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer ## use AutoTOkenizer will load the fast tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8824bbc-5bae-492a-8fa1-1e8826228382",
   "metadata": {},
   "source": [
    "- Here’s an example using the BERT tokenizer, which is a WordPiece tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5003be-ad7a-4cfe-8288-041c22cf87a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "print(tokenized_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd05434-b7f2-4a12-bd69-f565308ead49",
   "metadata": {},
   "source": [
    "- each token can be converted to IDs ; we can also decode from ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb84d9b-4a35-4a73-81a1-e67f41735a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]\n",
      "[CLS] A Titan RTX has 24GB of VRAM [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sequence,return_offsets_mapping=True)\n",
    "encoded_sequence = inputs['input_ids']\n",
    "print(encoded_sequence)\n",
    "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3ce06",
   "metadata": {},
   "source": [
    "- You can also track see if tokens are came from the same words and also the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c448ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M', '[SEP]']\n",
      "[None, 0, 1, 2, 2, 2, 3, 4, 4, 5, 6, 6, 6, None]\n",
      "[(0, 0), (0, 1), (2, 7), (8, 9), (9, 10), (10, 11), (12, 15), (16, 18), (18, 20), (21, 23), (24, 25), (25, 27), (27, 28), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())\n",
    "print(inputs.word_ids())\n",
    "print(inputs['offset_mapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5734c-7022-4155-bc3f-5ba09ec74a41",
   "metadata": {},
   "source": [
    "### Batch Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb966e6-7629-410e-84cc-eea6874d33f0",
   "metadata": {},
   "source": [
    "- Padding: Tokenizer will automatically pad a batch and provide proper attention masks\n",
    "- Truncation: On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length. Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model\n",
    "- Return Tensor: Set the return_tensors parameter to either pt for PyTorch, or tf for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d3ed80-c0b9-44de-ad00-897a26939c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1188, 1110,  170, 1603, 4954,  119,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1188, 1110,  170, 1897, 1263, 4954,  119, 1135, 1110, 1120, 1655,\n",
      "         2039, 1190, 1103, 4954,  138,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "padded_sequences = tokenizer([sequence_a, sequence_b], \n",
    "                             padding=True,truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d3b66-4e73-494b-b0e4-b2c7840c3f6f",
   "metadata": {},
   "source": [
    "### Processing sentence pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6602780e-e018-43d2-8bb7-803414e69700",
   "metadata": {},
   "source": [
    "- Some models’ purpose is to do classification on pairs of sentences or question answering.These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d237e6d6-3820-48cd-b1c6-ad62c1ff40ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sequence_a = \"HuggingFace is based in NYC\"\n",
    "sequence_b = \"Where is HuggingFace based?\"\n",
    "encoded_dict = tokenizer(sequence_a, sequence_b)\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
    "print(decoded)\n",
    "print(encoded_dict[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6074041-1614-47fa-9558-41fd1634f7d3",
   "metadata": {},
   "source": [
    "Small Tips :\n",
    "- For models employing the function apply_chunking_to_forward(), the chunk_size defines the number of output embeddings that are computed in parallel and thus defines the trade-off between memory and time complexity. If chunk_size is set to 0, no feed forward chunking is done.\n",
    "    https://huggingface.co/docs/transformers/glossary#feed-forward-chunking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3",
   "language": "python",
   "name": "gpt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
