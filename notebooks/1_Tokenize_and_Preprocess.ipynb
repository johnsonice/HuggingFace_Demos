{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5fea49-0548-4811-a6d3-015f5e287c38",
   "metadata": {},
   "source": [
    "### Tokenize and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca53ada0-7a0e-4e0c-9b87-111e7b80ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer ## use AutoTOkenizer will load the fast tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8824bbc-5bae-492a-8fa1-1e8826228382",
   "metadata": {},
   "source": [
    "- Here’s an example using the BERT tokenizer, which is a WordPiece tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5003be-ad7a-4cfe-8288-041c22cf87a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "print(tokenized_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd05434-b7f2-4a12-bd69-f565308ead49",
   "metadata": {},
   "source": [
    "- each token can be converted to IDs ; we can also decode from ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb84d9b-4a35-4a73-81a1-e67f41735a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]\n",
      "[CLS] A Titan RTX has 24GB of VRAM [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sequence,return_offsets_mapping=True)\n",
    "encoded_sequence = inputs['input_ids']\n",
    "print(encoded_sequence)\n",
    "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3ce06",
   "metadata": {},
   "source": [
    "- You can also track see if tokens are came from the same words and also the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c448ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M', '[SEP]']\n",
      "[None, 0, 1, 2, 2, 2, 3, 4, 4, 5, 6, 6, 6, None]\n",
      "[(0, 0), (0, 1), (2, 7), (8, 9), (9, 10), (10, 11), (12, 15), (16, 18), (18, 20), (21, 23), (24, 25), (25, 27), (27, 28), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())\n",
    "print(inputs.word_ids())\n",
    "print(inputs['offset_mapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5734c-7022-4155-bc3f-5ba09ec74a41",
   "metadata": {},
   "source": [
    "### Batch Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb966e6-7629-410e-84cc-eea6874d33f0",
   "metadata": {},
   "source": [
    "- Padding: Tokenizer will automatically pad a batch and provide proper attention masks\n",
    "- Truncation: On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length. Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model\n",
    "- Return Tensor: Set the return_tensors parameter to either pt for PyTorch, or tf for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d3ed80-c0b9-44de-ad00-897a26939c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1188, 1110,  170, 1603, 4954,  119,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1188, 1110,  170, 1897, 1263, 4954,  119, 1135, 1110, 1120, 1655,\n",
      "         2039, 1190, 1103, 4954,  138,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "padded_sequences = tokenizer([sequence_a, sequence_b], \n",
    "                             padding=True,truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d3b66-4e73-494b-b0e4-b2c7840c3f6f",
   "metadata": {},
   "source": [
    "### Processing sentence pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6602780e-e018-43d2-8bb7-803414e69700",
   "metadata": {},
   "source": [
    "- Some models’ purpose is to do classification on pairs of sentences or question answering.These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d237e6d6-3820-48cd-b1c6-ad62c1ff40ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sequence_a = \"HuggingFace is based in NYC\"\n",
    "sequence_b = \"Where is HuggingFace based?\"\n",
    "encoded_dict = tokenizer(sequence_a, sequence_b)\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
    "print(decoded)\n",
    "print(encoded_dict[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6074041-1614-47fa-9558-41fd1634f7d3",
   "metadata": {},
   "source": [
    "Small Tips :\n",
    "- For models employing the function apply_chunking_to_forward(), the chunk_size defines the number of output embeddings that are computed in parallel and thus defines the trade-off between memory and time complexity. If chunk_size is set to 0, no feed forward chunking is done.\n",
    "    https://huggingface.co/docs/transformers/glossary#feed-forward-chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039042db",
   "metadata": {},
   "source": [
    "### Customize Post Processing step \n",
    "- say we want to add additional features after the original text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4b1225e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "060b6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "972ae448",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'This is just an test sentence. with some posttive words like happy, excited; and some negetive words like however, angry, adverse'\n",
    "feature1 = 'happy excited'\n",
    "feature2 = 'however adverse'\n",
    "input_text2 = '{} [SEP] {}'.format(feature1,feature2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84267e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] This is just an test sentence. with some posttive words like happy, excited ; and some negetive words like however, angry, adverse [SEP]'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(input_text)\n",
    "tokenizer.decode(tokenizer.encode(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7005b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.vocab.get('[CLS]')\n",
    "sep_token_id = tokenizer.vocab.get('[SEP]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88145",
   "metadata": {},
   "source": [
    "- ### now we can change the post processing step function to formate customized formate\n",
    "- https://huggingface.co/docs/tokenizers/api/post-processors#tokenizers.processors.TemplateProcessing\n",
    "- https://huggingface.co/course/chapter6/8?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "382514d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer.post_processor = processors.TemplateProcessing(\\n    single=f\"[CLS]:0 $A:0 [SEP]:0\",\\n    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1 [EOS]:2\",\\n    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\\n)\\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### bert post processing looks something like this \n",
    "'''\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1 [EOS]:2\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7f7e636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's change it to something different, asusme a senario that we are provideing addtional feature/control as input 2\n",
    "tokenizer.backend_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=\"$A:0 [SEP]:0 $B:1 [SEP]:1 [CLS]:0\",  ## we changed the arrange, may not make sense but just as a demo\n",
    "    special_tokens=[(\"[SEP]\", sep_token_id), (\"[CLS]\", cls_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "47d2636c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is just an test sentence. with some posttive words like happy, excited ; and some negetive words like however, angry, adverse [SEP] happy excited [SEP] however adverse [SEP] [CLS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1188, 1110, 1198, 1126, 2774, 5650, 119, 1114, 1199, 2112, 3946, 1734, 1176, 2816, 117, 7215, 132, 1105, 1199, 24928, 16609, 2109, 1734, 1176, 1649, 117, 4259, 117, 16798, 102, 2816, 7215, 102, 1649, 16798, 102, 101], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now we can see the post processing it modified to customized way\n",
    "res = tokenizer(input_text,input_text2)\n",
    "print(tokenizer.decode(res['input_ids']))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "89d854e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../All_Data/HuggingFace/new_tokenizer/customized_post_process/tokenizer_config.json',\n",
       " '../../All_Data/HuggingFace/new_tokenizer/customized_post_process/special_tokens_map.json',\n",
       " '../../All_Data/HuggingFace/new_tokenizer/customized_post_process/vocab.txt',\n",
       " '../../All_Data/HuggingFace/new_tokenizer/customized_post_process/added_tokens.json',\n",
       " '../../All_Data/HuggingFace/new_tokenizer/customized_post_process/tokenizer.json')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "import os\n",
    "outdir = os.path.join(config.data_folder,'new_tokenizer','customized_post_process')\n",
    "tokenizer.save_pretrained(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bf2a6cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is just an test sentence. with some posttive words like happy, excited ; and some negetive words like however, angry, adverse [SEP] happy excited [SEP] however adverse [SEP] [CLS]\n"
     ]
    }
   ],
   "source": [
    "## to load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(outdir)\n",
    "res = tokenizer(input_text,input_text2)\n",
    "print(tokenizer.decode(res['input_ids'])) ## load it and it is correctly tokenizing inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0bb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt3",
   "language": "python",
   "name": "gpt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
