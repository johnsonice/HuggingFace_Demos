{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting start with Bloom"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "- https://towardsdatascience.com/getting-started-with-bloom-9e3295459b65\n",
    "- https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu.huang/anaconda3/envs/sbert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "import os, sys , ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 688/688 [00:00<00:00, 254kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 5.59G/5.59G [07:31<00:00, 13.3MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 222/222 [00:00<00:00, 76.5kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 13.8M/13.8M [00:00<00:00, 34.4MB/s]\n",
      "Downloading special_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 33.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "## for now, we will just use the 3 b model \n",
    "cache_dir = '/data/chuang/temp'\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-3b\",cache_dir=cache_dir)\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-3b\",cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generate python code to loop through a list of 5 items and change their values:\"\n",
    "result_length = 500 \n",
    "inputs = tokenizer(prompt,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate python code to loop through a list of 5 items and change their values:\n",
      "import random\n",
      "import time\n",
      "\n",
      "def change_item(item):\n",
      "    print(\"Item: \" + item)\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\" \", \"\")\n",
      "    item = item.replace(\",\", \"\")\n",
      "    item = item.replace(\".\", \"\")\n",
      "    item = item.replace(\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_length=result_length\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate python code to loop through a dataframe:\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.DataFrame({'A':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,\n"
     ]
    }
   ],
   "source": [
    "# Beam Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate python code to loop through a dataframe:\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.DataFrame({\n",
      "   'sample': [\n",
      "        {'A': 'AA', 'B': 'BB'},\n",
      "        {'A': 'AA', 'B': 'AA'},\n",
      "        {'A': 'AA', 'B': 'AA'},\n",
      "        {'A': 'AA', 'B': 'AA'},\n",
      "        {'A': 'AA', 'B': 'AA'},\n",
      "        {'A': 'AA', 'B': 'AA'},\n",
      "    ],\n",
      "    'N': [\n",
      "        {'X': 1, 'Y': 2},\n",
      "        {'X': 1, 'Y': 3},\n",
      "        {'X': 2, 'Y': 2},\n",
      "        {'X': 3, 'Y': 2},\n",
      "        {'X': 3, 'Y': 3},\n",
      "    ],\n",
      "    'Z': [\n",
      "        {'X': 1, 'Y': 1},\n",
      "        {'X': 1, 'Y': 1},\n",
      "        {'X': 1, 'Y': 1},\n",
      "        {'X': 1, 'Y': 1},\n",
      "        {'X': 2, 'Y': 1},\n",
      "        {'X': 2, 'Y': 1},\n",
      "    ]\n",
      "})\n",
      "\n",
      "df.to_sql('df_tst', connection='postgres')\n",
      "pd.read_sql_query('df_tst'.to_sql('SELECT * FROM df_tst', connection='postgres'), output_format='list')\n",
      "\n",
      "How do I loop through the dataframe using pandas to calculate the sum of the rows, the maximum value and the minimum value. The result should look like this.\n",
      "\n",
      "The expected output should be:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use.set_index and then.iloc and.cumsum to create a new DataFrame:\n",
      "def func(df):\n",
      "    df_sum = df.set_index(['A', 'N']).iloc[:,0].cumsum()\n",
      "    df_max = df.set_index(['A', 'N']).iloc[:,1].max()\n",
      "    df_min = df.set_index(['A', 'N']).iloc[:,1].min()\n",
      "\n",
      "    return (df['X']+df['Y'])/(df['X']+df['Y']).replace(np.nan, np.nan).astype(np.float32)\n",
      "\n",
      "df1 = df.set_index(['A', 'N']).iloc[:,0].cumsum()\n",
      "df2 = df.set_index(['A', 'N']).iloc[:,1].cumsum()\n",
      "df_result = pd.concat([df1\n"
     ]
    }
   ],
   "source": [
    "# Sampling Top-k + Top-p\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6cf887a0cd63b1646b72eb9cdb014d6c9733a8a0a6989a432f0f7542bfe6547e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 ('sbert': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44cb1e07817c37f4eeaa27cde36437eb6d39fa76886fb902edddf9788ba50049"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
